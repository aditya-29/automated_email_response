{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi! I'm just an AI, so I don't have feelings or emotions like humans do, but I'm functioning properly and ready to help with any questions or tasks you may have! How about you, how's your day going?\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "class Chat:\n",
    "    def __init__(self,\n",
    "                api_key_path = \"./api_key\",\n",
    "                model=\"llama3-8b-8192\", \n",
    "                temperature=0,\n",
    "                max_tokens=2048,\n",
    "                top_p=1,\n",
    "                stop=None,\n",
    "                stream=False,\n",
    "                ):\n",
    "        # groq parameters\n",
    "        self.api_key_path = api_key_path\n",
    "        self.model = model\n",
    "\n",
    "        # model parameters\n",
    "        self.__temperature = temperature\n",
    "        self.__max_tokens = max_tokens\n",
    "        self.__top_p = top_p\n",
    "        self.__stop = stop\n",
    "        self.__stream = stream\n",
    "\n",
    "        # read api key\n",
    "        self.read_api_key()\n",
    "\n",
    "    def read_api_key(self):\n",
    "        with open(self.api_key_path, \"r\") as f:\n",
    "            api_key = f.readline()\n",
    "        self.client = Groq(api_key=api_key)\n",
    "\n",
    "    def __construct_messages__(self, user_msgs, system_msg=None, assistant_msgs=None):\n",
    "        messages = []\n",
    "\n",
    "        if system_msg:\n",
    "            msg = {\n",
    "                \"role\" : \"system\",\n",
    "                \"content\" : system_msg\n",
    "            }\n",
    "            messages.append(msg)\n",
    "\n",
    "        while user_msgs:\n",
    "            cur_user_msg = user_msgs.pop(0)\n",
    "            user_msg = {\n",
    "                \"role\" : \"user\",\n",
    "                \"content\" : cur_user_msg\n",
    "            }\n",
    "            messages.append(user_msg)\n",
    "\n",
    "            if assistant_msgs:\n",
    "                cur_assisant_msg = assistant_msgs.pop(0)\n",
    "                assistant_msg = {\n",
    "                    \"role\" : \"assistant\",\n",
    "                    \"content\" : cur_assisant_msg\n",
    "                }\n",
    "                messages.append(assistant_msg)\n",
    "\n",
    "        return messages\n",
    "            \n",
    "\n",
    "    def chat(self, \n",
    "            user_msgs:list[str], \n",
    "            system_msg:str = None, \n",
    "            assistant_msgs:list[str] = None,\n",
    "            _temperature:float=None,\n",
    "            _max_tokens:int=None,\n",
    "            _top_p:float=None,\n",
    "            _stop:str=None,\n",
    "            _stream:bool=None):\n",
    "        \n",
    "        # set defaults\n",
    "        _temperatue = _temperature if _temperature is not None else self.__temperature\n",
    "        _max_tokens = _max_tokens if _max_tokens is not None else self.__max_tokens\n",
    "        _top_p = _top_p if _top_p is not None else self.__top_p\n",
    "        _stop = _stop if _stop is not None else self.__stop\n",
    "        _stream = _stream if _stream is not None else self.__stream\n",
    "\n",
    "        # construct the messages\n",
    "        messages = self.__construct_messages__(user_msgs=user_msgs, system_msg=system_msg, assistant_msgs=assistant_msgs)\n",
    "        \n",
    "        # get the chat completion\n",
    "        chat_completion = self.client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            model=self.model,\n",
    "            temperature=_temperatue,\n",
    "            max_tokens=_max_tokens,\n",
    "            top_p=_top_p,\n",
    "            stop=_stop,\n",
    "            stream=_stream\n",
    "        )\n",
    "\n",
    "        # retrive the outputs\n",
    "        __reason_for_stop__ = chat_completion.choices[0].finish_reason\n",
    "        output = chat_completion.choices[0].message.content\n",
    "\n",
    "        # handle exceptions\n",
    "        if __reason_for_stop__ != \"stop\":\n",
    "            print(\"[COLCA-WARNING] The chat is stopped because of token limit exceeded : \", __reason_for_stop__)\n",
    "\n",
    "        return output\n",
    "\n",
    "c = Chat()\n",
    "c.chat([\"hi, how are you?\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an AI salesperson with specialized knowledge limited to the provided context. Your responses should strictly adhere to the information given and refrain from any outside knowledge, assumptions, or invented details. When responding to a potential lead: \n",
      "Answer questions concisely, only drawing from the provided context. \n",
      "Avoid any phrases or context that extend beyond the exact information provided. \n",
      "If a question requires knowledge outside of the given facts, respond with: 'Sorry, I cannot help you with that'. Avoid adding any additional wording or elaboration.\n",
      "\n",
      "\n",
      "Context: this is the context\n",
      "\n",
      "Question: this is the question\n",
      "\n",
      "Reply: \n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def wrap(string):\n",
    "        # string = reduce(lambda x, y : x+y, string)\n",
    "        string = string.replace(\"  \", \"\")\n",
    "        string = string.replace(\"\\t\\n\", \"\\t\")\n",
    "        string = string.replace(\"\\t\", \"\")\n",
    "        return string\n",
    "\n",
    "class Prompts:\n",
    "    class Summarize:\n",
    "        SYSTEM_MESSAGE = wrap(\"\"\"You are an AI summarizer, your job is to summarize the piece of text based on the given title \\\n",
    "                        and give a short hand form of them with the important keywords in it. Make sure to format the summarized text \\\n",
    "                        in such a way that an AI model can understand and can use it as a prompt. Do Not include any words other the summarized \\\n",
    "                        text. Do not include any unnecessary lines in the response\"\"\")\n",
    "        \n",
    "        GUARD_RAILS = wrap(\"\"\"Summarize the above titles and their content. Make sure to include only the summarized text and nothing else\n",
    "                                    1. Extract only core information directly relevant to the title or topic.\n",
    "                                    2. Prioritize main points, essential keywords, and relevant details.\n",
    "                                    3. Avoid additional explanations, background context, or filler words.\n",
    "                                    4. Include only high-value, specific keywords (e.g., names, locations, numbers, technical terms).\n",
    "                                    5. Exclude redundant or less relevant terms that do not contribute to the prompt's focus.\n",
    "                                    6. Use bullet points or brief statements when possible.\n",
    "                                    7. Maintain concise, structured phrasing, avoiding full sentences if not necessary.\n",
    "                                    8. Avoid adding extra formatting or punctuation unless it enhances clarity for an AI model.\n",
    "                                    9. Ensure that the final output can be understood by an AI model as a standalone prompt.\n",
    "                                    10. Avoid words like \"summary,\" \"in summary,\" or other introductory or concluding phrases.\n",
    "                                    11. Aim for brevity; convey information in the shortest, most direct form possible.\n",
    "                                    12. Use abbreviations and shorthand when appropriate and easily understandable by an AI model.\n",
    "                                    13. Ensure that the summarized text accurately reflects the original text's intent and meaning.\n",
    "                                    14. Avoid altering or misinterpreting any critical information or nuances.\n",
    "                                    15. No extra instructions, explanations, or framing text around the response.\n",
    "                                    16. Avoid system-generated phrases or additional prompts beyond the summarization requirements.\n",
    "                                   \"\"\")\n",
    "        \n",
    "        QA_SYSTEM_MSG = wrap(\"\"\"You are an AI summarizer, your job is to summarize the lead information along with the questions they asked and the answer generated \\\n",
    "                             by an AI system. Make sure to format the summarized text in such a way that it easy for a sales manager to understand the history of conversation \\\n",
    "                             between the AI system and the lead who asked the actual answers. Do not include any words other the summarized text. Do not include any \\\n",
    "                             unnecessary lines in the response. Your summary should definitely include the questions asked and the response generated, you can list them out.\"\"\")\n",
    "        \n",
    "    class QA:\n",
    "        QUESTION_JUDGE = wrap(\"\"\"You are an AI assistant, your job is to validate a question asked by a user based on the provided context. \\\n",
    "                                      If the question is not relevant to the context at hand, respond \"invalid\", if its relevant then respond \"valid\". \\\n",
    "                                      Do not incldue any other words except \"valid\" or \"invalid\" in your response. The question will be provided by the user \\\n",
    "                                      so act as a judge in this scenario \\n\\nContext: {context} \\n\\nQuestion: {question} \\n\\nResponse: \"\"\")\n",
    "        \n",
    "        DEFAULT_RESPONSE = \"Sorry, I cannot help you with that\"\n",
    "\n",
    "        QUESTION_REPLY = wrap(\"\"\"You are an AI salesperson with specialized knowledge limited to the provided context. \\\n",
    "                                    Your responses should strictly adhere to the information given and refrain from any outside knowledge, assumptions, or invented details. \\\n",
    "                                    When responding to a potential lead: \n",
    "                                    Answer questions concisely, only drawing from the provided context. \n",
    "                                    Avoid any phrases or context that extend beyond the exact information provided. \n",
    "                                    If a question requires knowledge outside of the given facts, respond with: '\"\"\"+ DEFAULT_RESPONSE + \"\"\"'. \\\n",
    "                                    Avoid adding any additional wording or elaboration.\n",
    "                                    \\n\\nContext: {context}\\n\\nQuestion: {question}\\n\\nReply: \"\"\")\n",
    "\n",
    "P = Prompts \n",
    "print(P.QA.QUESTION_REPLY.format(context=\"this is the context\", question=\"this is the question\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summarizer:\n",
    "    def __init__(self,\n",
    "                chat:Chat,\n",
    "                ):\n",
    "        self.chat = chat\n",
    "    \n",
    "    def summarize_email_product(self, documents:dict,):\n",
    "        # retrive the necessary prompts\n",
    "        sys_msg = Prompts.Summarize.SYSTEM_MESSAGE\n",
    "        guard_rails = Prompts.Summarize.GUARD_RAILS\n",
    "\n",
    "        # create the message content\n",
    "        message = \"\"\n",
    "        for title, content in documents.items():\n",
    "            cur_msg = f\"title : {title}\\n\\n\"\n",
    "            cur_msg += f\"content : {content}\\n\\n\"\n",
    "            message += cur_msg\n",
    "\n",
    "        message += guard_rails\n",
    "\n",
    "        # pass it through the chat\n",
    "        output = self.chat.chat(system_msg=sys_msg,\n",
    "                                user_msgs=[message])\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def summarize_qa_lead(self, qa:tuple[str, str], lead_info: str):\n",
    "        # retrive the necessary prompts\n",
    "        sys_msg = Prompts.Summarize.QA_SYSTEM_MSG\n",
    "        guard_rails = Prompts.Summarize.GUARD_RAILS\n",
    "\n",
    "        # create the message content\n",
    "        message = \"Lead Info : \" + lead_info + \"\\n\\n\"\n",
    "        \n",
    "        for question, response in qa:\n",
    "            if response == Prompts.QA.DEFAULT_RESPONSE:\n",
    "                continue\n",
    "            message += \"Question Asked by Lead : \" + question + \"\\nAI Generated Response : \" + response + \"\\n\\n\"\n",
    "\n",
    "        message += guard_rails\n",
    "\n",
    "        # pass it through the chat\n",
    "        output = self.chat.chat(system_msg=sys_msg,\n",
    "                                user_msgs=[message])\n",
    "        \n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample:\n",
    "    def __init__(self):\n",
    "        with open(\"./sample/sample_company_information.txt\", \"r\") as f:\n",
    "            self.product_info = f.read()\n",
    "\n",
    "        with open(\"./sample/sample_email_content.txt\", \"r\") as f:\n",
    "            self.email_content = f.read()\n",
    "\n",
    "        with open(\"./sample/sample_lead_info.txt\", \"r\") as f:\n",
    "            self.lead_info = f.read()\n",
    "\n",
    "\n",
    "class ReplyMachine:\n",
    "    def __init__(self, \n",
    "                product_information: str,\n",
    "                email_content: str,\n",
    "                chat: Chat,\n",
    "                lead_info: str,\n",
    "                ):      \n",
    "        self.chat = chat\n",
    "        self.product_information = product_information\n",
    "        self.email_content = email_content\n",
    "        self.lead_info = lead_info\n",
    "        self.text_summarized_email_product = None\n",
    "        self.questions_asked = []\n",
    "        self.qa_threads = []\n",
    "\n",
    "    def summarize_email_and_product(self):\n",
    "        # initialize a summarizer\n",
    "        summarizer = Summarizer(self.chat)\n",
    "\n",
    "        # create document dict\n",
    "        documents = {}\n",
    "        documents[\"Product Information\"] = self.product_information\n",
    "        documents[\"Email Content\"] = self.email_content\n",
    "\n",
    "        # summarize\n",
    "        summarized_text = summarizer.summarize_email_product(documents=documents)\n",
    "        self.text_summarized_email_product = summarized_text\n",
    "        print(\"[INFO] Summarized Email Content and Product Information\")\n",
    "\n",
    "    def __summarize_qa_and_lead_info(self):\n",
    "        # initialize the summarizer\n",
    "        summarizer = Summarizer(self.chat)\n",
    "\n",
    "        # summarize\n",
    "        summarized_text = summarizer.summarize_qa_lead(qa = self.qa_threads, lead_info=self.lead_info)\n",
    "        return summarized_text\n",
    "\n",
    "        \n",
    "\n",
    "    def __validate_question(self, question):\n",
    "        # get the prompt\n",
    "        message = Prompts.QA.QUESTION_JUDGE.format(question=question, context=self.product_information)\n",
    "\n",
    "        # generate a valid/invalid for the question\n",
    "        output = self.chat.chat(user_msgs=[message])\n",
    "\n",
    "        if \"valid\" in output.split(\" \"):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    def reply(self, question):\n",
    "        \n",
    "        # generate product information and email summary if not generated already\n",
    "        if self.text_summarized_email_product is None:\n",
    "            self.summarize_email_and_product()\n",
    "\n",
    "        # log the questions\n",
    "        self.questions_asked.append(question)\n",
    "\n",
    "        # validate the question\n",
    "        validation = self.__validate_question(question)\n",
    "        \n",
    "        if not validation:\n",
    "            print(\"[COLCA-ERROR] The asked question is not valid : \", question)\n",
    "            self.qa_threads.append((question,Prompts.QA.DEFAULT_RESPONSE))\n",
    "            return Prompts.QA.DEFAULT_RESPONSE\n",
    "\n",
    "        # get the prompt\n",
    "        message = Prompts.QA.QUESTION_REPLY.format(question=question, context=self.text_summarized_email_product)\n",
    "\n",
    "        # generate the output from the model\n",
    "        output = self.chat.chat(user_msgs=[message])\n",
    "\n",
    "        # add it to the thread\n",
    "        self.qa_threads.append((question, output))\n",
    "       \n",
    "        return output\n",
    "    \n",
    "    def quick_bytes(self):\n",
    "        summaried_qa_lead = self.__summarize_qa_and_lead_info()\n",
    "        return summaried_qa_lead\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = Sample()\n",
    "chat = Chat()\n",
    "\n",
    "R = ReplyMachine(\n",
    "    product_information=sample.product_info,\n",
    "    email_content=sample.email_content,\n",
    "    chat=chat,\n",
    "    lead_info=sample.lead_info\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------  Question  --------------------\n",
      "Hi, How are you?\n",
      "ans : \n",
      "[COLCA-ERROR] The asked question is not valid :  Hi, How are you?\n",
      "Sorry, I cannot help you with that\n",
      "--------------------  Question  --------------------\n",
      "How many cameras does iphone 15 pro has?\n",
      "ans : \n",
      "The iPhone 15 Pro has 3 cameras: 48MP wide sensor, 12MP telephoto lens, and 12MP ultra-wide lens.\n",
      "--------------------  Question  --------------------\n",
      "How many usb ports does the iphone 15 pro has?\n",
      "ans : \n",
      "The iPhone 15 Pro has 1 USB-C port.\n"
     ]
    }
   ],
   "source": [
    "question = \"Hi, How are you?\"\n",
    "print(\"-\"*20, \" Question \", \"-\"*20)\n",
    "print(question)\n",
    "print(\"ans : \")\n",
    "print(R.reply(question))\n",
    "\n",
    "question = \"How many cameras does iphone 15 pro has?\"\n",
    "print(\"-\"*20, \" Question \", \"-\"*20)\n",
    "print(question)\n",
    "print(\"ans : \")\n",
    "print(R.reply(question))\n",
    "\n",
    "question = \"How many usb ports does the iphone 15 pro has?\"\n",
    "print(\"-\"*20, \" Question \", \"-\"*20)\n",
    "print(question)\n",
    "print(\"ans : \")\n",
    "print(R.reply(question))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hi, How are you?', 'Sorry, I cannot help you with that'), ('How many cameras does iphone 15 pro has?', 'The iPhone 15 Pro has 3 cameras: 48MP wide sensor, 12MP telephoto lens, and 12MP ultra-wide lens.'), ('How many usb ports does the iphone 15 pro has?', 'The iPhone 15 Pro has 1 USB-C port.'), ('Hi, How are you?', 'Sorry, I cannot help you with that'), ('How many cameras does iphone 15 pro has?', 'The iPhone 15 Pro has 3 cameras: a 48MP wide sensor, a 12MP telephoto lens, and a 12MP ultra-wide lens.'), ('How many usb ports does the iphone 15 pro has?', 'The iPhone 15 Pro has 1 USB-C port.'), ('Hi, How are you?', 'Sorry, I cannot help you with that'), ('How many cameras does iphone 15 pro has?', 'The iPhone 15 Pro has 3 cameras: 48MP wide sensor, 12MP telephoto lens, and 12MP ultra-wide lens.'), ('How many usb ports does the iphone 15 pro has?', 'The iPhone 15 Pro has 1 USB-C port.')]\n",
      "Lead Information:\n",
      "• Name: John Doe\n",
      "• Profession: Tech Enthusiast / IT Professional\n",
      "• Location: India\n",
      "• Interests: Technology, Mobile Devices, Latest Gadgets\n",
      "• Device Preference: Prefers premium, high-performance devices, enjoys exploring new features, and is an early adopter of advanced tech like AI, 5G, and high-quality cameras.\n",
      "• Potential Need: Interested in the iPhone 15 Pro for its cutting-edge A17 Pro chip, superior camera system, and robust design for tech-savvy users.\n",
      "\n",
      "Questions Asked by Lead:\n",
      "• How many cameras does iPhone 15 Pro have?\n",
      "• How many USB ports does the iPhone 15 Pro have?\n",
      "• How many cameras does iPhone 15 Pro have? (repeated)\n",
      "• How many USB ports does the iPhone 15 Pro have? (repeated)\n",
      "\n",
      "AI Generated Responses:\n",
      "• The iPhone 15 Pro has 3 cameras: 48MP wide sensor, 12MP telephoto lens, and 12MP ultra-wide lens.\n",
      "• The iPhone 15 Pro has 1 USB-C port.\n",
      "• The iPhone 15 Pro has 3 cameras: a 48MP wide sensor, a 12MP telephoto lens, and a 12MP ultra-wide lens.\n",
      "• The iPhone 15 Pro has 1 USB-C port.\n"
     ]
    }
   ],
   "source": [
    "print(R.qa_threads)\n",
    "\n",
    "print(R.quick_bytes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 04:01:50.621 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/aditya/Library/Python/3.10/lib/python/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "\n",
    "# Title of the app\n",
    "st.title('My First Streamlit App')\n",
    "\n",
    "# Display a simple text\n",
    "st.write('Hello, welcome to my first Streamlit app!')\n",
    "\n",
    "# Input widget\n",
    "name = st.text_input('Enter your name')\n",
    "\n",
    "# Button widget\n",
    "if st.button('Say Hello'):\n",
    "    st.write(f'Hello, {name}!')\n",
    "    \n",
    "# Add a slider\n",
    "age = st.slider('Select your age', 0, 100, 25)\n",
    "\n",
    "# Display the age\n",
    "st.write(f'Your age is: {age}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
